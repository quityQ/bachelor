{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, using scikit-learn and statsmodels (based on introduction to statistical learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import statsmodels.api as sm \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "large = pd.read_csv('../datasets/full_cleaned_dataset.csv')\n",
    "small = pd.read_csv('../datasets/1std_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'#turn dfs in numpy arrays\\nlarge = large.to_numpy()\\nsmall = small.to_numpy()'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#dropping the z_score column, so that small is a subset of large\n",
    "small.drop(columns=['z_score'], inplace=True) \n",
    "\n",
    "#drop any non-numeric colums from both datasets\n",
    "#getting lists of numeric columns\n",
    "numeric_columns = large.select_dtypes(include=np.number).columns\n",
    "\n",
    "#dropping non-numeric columns from large and small datasets\n",
    "large = large[numeric_columns]\n",
    "small = small[numeric_columns]\n",
    "\n",
    "\"\"\"#turn dfs in numpy arrays\n",
    "large = large.to_numpy()\n",
    "small = small.to_numpy()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.0\n",
      "1      0.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "      ... \n",
      "662    1.0\n",
      "663    1.0\n",
      "664    1.0\n",
      "665    1.0\n",
      "666    1.0\n",
      "Name: distressed, Length: 667, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(small['distressed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:             distressed   No. Observations:                  667\n",
      "Model:                            GLM   Df Residuals:                      564\n",
      "Model Family:                Binomial   Df Model:                          102\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -2.4333e-09\n",
      "Date:                Fri, 04 Apr 2025   Deviance:                   4.8667e-09\n",
      "Time:                        16:52:07   Pearson chi2:                 2.43e-09\n",
      "No. Iterations:                    27   Pseudo R-squ. (CS):             0.1219\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0004   1.56e+04  -2.56e-08      1.000   -3.06e+04    3.06e+04\n",
      "x2            -0.2098   6.38e+05  -3.29e-07      1.000   -1.25e+06    1.25e+06\n",
      "x3             0.0490   8.05e+04   6.09e-07      1.000   -1.58e+05    1.58e+05\n",
      "x4            -0.1332   6.37e+05  -2.09e-07      1.000   -1.25e+06    1.25e+06\n",
      "x5             0.0086   1.01e+05    8.5e-08      1.000   -1.98e+05    1.98e+05\n",
      "x6            -0.2606   6.26e+05  -4.16e-07      1.000   -1.23e+06    1.23e+06\n",
      "x7            -0.1329   2.78e+05  -4.79e-07      1.000   -5.44e+05    5.44e+05\n",
      "x8             0.1954    5.4e+05   3.62e-07      1.000   -1.06e+06    1.06e+06\n",
      "x9             0.0285   1.18e+05   2.42e-07      1.000   -2.31e+05    2.31e+05\n",
      "x10            0.0890   8.81e+04   1.01e-06      1.000   -1.73e+05    1.73e+05\n",
      "x11            0.0190   7.89e+04   2.41e-07      1.000   -1.55e+05    1.55e+05\n",
      "x12           -0.0513   1.33e+05  -3.86e-07      1.000   -2.61e+05    2.61e+05\n",
      "x13           -0.0797   1.08e+06  -7.36e-08      1.000   -2.12e+06    2.12e+06\n",
      "x14           -0.0003   4.95e+05     -6e-10      1.000   -9.71e+05    9.71e+05\n",
      "x15            0.0727   6.31e+05   1.15e-07      1.000   -1.24e+06    1.24e+06\n",
      "x16            0.0613   7.05e+05    8.7e-08      1.000   -1.38e+06    1.38e+06\n",
      "x17           -0.0386   5.56e+05  -6.94e-08      1.000   -1.09e+06    1.09e+06\n",
      "x18            0.3179   2.28e+06   1.39e-07      1.000   -4.47e+06    4.47e+06\n",
      "x19           -0.0092   1.03e+05  -8.97e-08      1.000   -2.01e+05    2.01e+05\n",
      "x20            0.0189    6.6e+04   2.87e-07      1.000   -1.29e+05    1.29e+05\n",
      "x21            0.0016   4.71e+04   3.32e-08      1.000   -9.23e+04    9.23e+04\n",
      "x22           -0.0003   2.15e+04  -1.56e-08      1.000   -4.22e+04    4.22e+04\n",
      "x23            0.0032   7.37e+04   4.39e-08      1.000   -1.44e+05    1.44e+05\n",
      "x24           -0.0485   9.59e+04  -5.06e-07      1.000   -1.88e+05    1.88e+05\n",
      "x25            0.0003   3.23e+05   8.72e-10      1.000   -6.33e+05    6.33e+05\n",
      "x26            0.0059   2.57e+04   2.29e-07      1.000   -5.03e+04    5.03e+04\n",
      "x27           -0.0112   3.77e+04  -2.96e-07      1.000   -7.38e+04    7.38e+04\n",
      "x28           -0.0662   1.62e+05   -4.1e-07      1.000   -3.17e+05    3.17e+05\n",
      "x29           -0.1137   4.65e+05  -2.45e-07      1.000   -9.11e+05    9.11e+05\n",
      "x30            0.0058    4.8e+05   1.22e-08      1.000   -9.41e+05    9.41e+05\n",
      "x31           -0.0058   3.08e+04   -1.9e-07      1.000   -6.05e+04    6.05e+04\n",
      "x32            0.0359   8.89e+05   4.04e-08      1.000   -1.74e+06    1.74e+06\n",
      "x33           -0.0241    3.9e+04  -6.17e-07      1.000   -7.64e+04    7.64e+04\n",
      "x34           -0.0153   3.73e+04  -4.12e-07      1.000    -7.3e+04     7.3e+04\n",
      "x35            0.0191   9.76e+04   1.96e-07      1.000   -1.91e+05    1.91e+05\n",
      "x36            0.0101   1.14e+05   8.86e-08      1.000   -2.22e+05    2.22e+05\n",
      "x37            0.0277   6.36e+04   4.36e-07      1.000   -1.25e+05    1.25e+05\n",
      "x38           -0.0290   8.48e+04  -3.43e-07      1.000   -1.66e+05    1.66e+05\n",
      "x39           -0.0287   8.39e+04  -3.42e-07      1.000   -1.65e+05    1.65e+05\n",
      "x40           -0.1255   9.58e+05  -1.31e-07      1.000   -1.88e+06    1.88e+06\n",
      "x41            0.0022   3.98e+04   5.49e-08      1.000    -7.8e+04     7.8e+04\n",
      "x42           -0.1255   9.58e+05  -1.31e-07      1.000   -1.88e+06    1.88e+06\n",
      "x43            0.0082   1.13e+06   7.22e-09      1.000   -2.22e+06    2.22e+06\n",
      "x44            0.2468   8.11e+05   3.04e-07      1.000   -1.59e+06    1.59e+06\n",
      "x45           -0.2002    5.8e+05  -3.45e-07      1.000   -1.14e+06    1.14e+06\n",
      "x46           -0.1269    9.7e+05  -1.31e-07      1.000    -1.9e+06     1.9e+06\n",
      "x47           -0.0316    2.5e+05  -1.26e-07      1.000   -4.91e+05    4.91e+05\n",
      "x48            0.0169   8.35e+04   2.03e-07      1.000   -1.64e+05    1.64e+05\n",
      "x49            0.1134   8.35e+05   1.36e-07      1.000   -1.64e+06    1.64e+06\n",
      "x50           -0.0784   2.78e+06  -2.83e-08      1.000   -5.44e+06    5.44e+06\n",
      "x51           -0.0002   2.21e+05  -1.05e-09      1.000   -4.33e+05    4.33e+05\n",
      "x52            0.0111   2.19e+05   5.06e-08      1.000    -4.3e+05     4.3e+05\n",
      "x53            0.0110   2.07e+05   5.31e-08      1.000   -4.06e+05    4.06e+05\n",
      "x54            0.1003    2.7e+06   3.71e-08      1.000   -5.29e+06    5.29e+06\n",
      "x55           -0.0202   8.35e+05  -2.43e-08      1.000   -1.64e+06    1.64e+06\n",
      "x56           -0.0372   2.26e+06  -1.64e-08      1.000   -4.44e+06    4.44e+06\n",
      "x57           -0.0044   2.76e+05  -1.58e-08      1.000   -5.41e+05    5.41e+05\n",
      "x58            0.0248   2.57e+05   9.65e-08      1.000   -5.04e+05    5.04e+05\n",
      "x59            0.1078   1.02e+06   1.06e-07      1.000      -2e+06       2e+06\n",
      "x60            0.0874   9.86e+05   8.87e-08      1.000   -1.93e+06    1.93e+06\n",
      "x61           -0.0073   1.08e+06   -6.8e-09      1.000   -2.11e+06    2.11e+06\n",
      "x62           -0.1490   1.15e+06   -1.3e-07      1.000   -2.25e+06    2.25e+06\n",
      "x63           -0.0159   1.07e+05  -1.49e-07      1.000    -2.1e+05     2.1e+05\n",
      "x64           -0.0180   1.41e+05  -1.27e-07      1.000   -2.77e+05    2.77e+05\n",
      "x65           -0.0047   7.85e+04  -6.04e-08      1.000   -1.54e+05    1.54e+05\n",
      "x66           -0.0050    1.3e+05  -3.83e-08      1.000   -2.55e+05    2.55e+05\n",
      "x67           -0.2000   2.44e+05   -8.2e-07      1.000   -4.78e+05    4.78e+05\n",
      "x68           -0.1035   4.15e+05   -2.5e-07      1.000   -8.13e+05    8.13e+05\n",
      "x69           -0.0051   3.55e+04  -1.45e-07      1.000   -6.96e+04    6.96e+04\n",
      "x70           -0.0491   2.22e+07  -2.22e-09      1.000   -4.34e+07    4.34e+07\n",
      "x71            0.5390   6.07e+07   8.88e-09      1.000   -1.19e+08    1.19e+08\n",
      "x72           -0.3845   5.05e+07  -7.62e-09      1.000   -9.89e+07    9.89e+07\n",
      "x73            0.5592   2.65e+07   2.11e-08      1.000    -5.2e+07     5.2e+07\n",
      "x74            0.0972    5.5e+06   1.77e-08      1.000   -1.08e+07    1.08e+07\n",
      "x75           -0.5109   2.48e+07  -2.06e-08      1.000   -4.86e+07    4.86e+07\n",
      "x76           -0.0397   8.57e+04  -4.64e-07      1.000   -1.68e+05    1.68e+05\n",
      "x77            0.0079   7.06e+04   1.12e-07      1.000   -1.38e+05    1.38e+05\n",
      "x78           -0.0599   1.31e+05  -4.56e-07      1.000   -2.57e+05    2.57e+05\n",
      "x79           -0.0008   1.95e+04  -4.13e-08      1.000   -3.83e+04    3.83e+04\n",
      "x80           -0.0243   8.24e+04  -2.95e-07      1.000   -1.62e+05    1.62e+05\n",
      "x81         7.976e-05   4.71e+04   1.69e-09      1.000   -9.23e+04    9.23e+04\n",
      "x82           -0.0043   3.31e+04   -1.3e-07      1.000   -6.48e+04    6.48e+04\n",
      "x83           -0.0646   1.46e+05  -4.42e-07      1.000   -2.86e+05    2.86e+05\n",
      "x84           -0.0517   1.53e+05  -3.38e-07      1.000      -3e+05       3e+05\n",
      "x85            0.0710   2.25e+05   3.16e-07      1.000    -4.4e+05     4.4e+05\n",
      "x86           -0.0014   1.11e+05  -1.25e-08      1.000   -2.18e+05    2.18e+05\n",
      "x87           -0.0660   3.46e+05  -1.91e-07      1.000   -6.78e+05    6.78e+05\n",
      "x88           -0.0209   3.42e+05  -6.11e-08      1.000   -6.71e+05    6.71e+05\n",
      "x89            0.0589   2.15e+05   2.74e-07      1.000   -4.21e+05    4.21e+05\n",
      "x90           -0.2931   6.25e+05  -4.69e-07      1.000   -1.23e+06    1.23e+06\n",
      "x91           -0.0416   6.59e+04  -6.31e-07      1.000   -1.29e+05    1.29e+05\n",
      "x92            0.0140   2.26e+05   6.19e-08      1.000   -4.43e+05    4.43e+05\n",
      "x93            0.0053   5.21e+04   1.02e-07      1.000   -1.02e+05    1.02e+05\n",
      "x94           -0.0037   1.13e+05  -3.24e-08      1.000   -2.22e+05    2.22e+05\n",
      "x95            0.1632   6.51e+05   2.51e-07      1.000   -1.28e+06    1.28e+06\n",
      "x96            0.0740   2.17e+05   3.41e-07      1.000   -4.26e+05    4.26e+05\n",
      "x97            0.0125   8.33e+04   1.49e-07      1.000   -1.63e+05    1.63e+05\n",
      "x98            0.1491   3.77e+05   3.95e-07      1.000    -7.4e+05     7.4e+05\n",
      "x99           -0.0358   2.34e+05  -1.53e-07      1.000   -4.59e+05    4.59e+05\n",
      "x100        -184.4109    2.5e+09  -7.37e-08      1.000    -4.9e+09     4.9e+09\n",
      "x101         184.5539    2.5e+09   7.37e-08      1.000    -4.9e+09     4.9e+09\n",
      "x102          -0.0280   2.33e+04  -1.21e-06      1.000   -4.56e+04    4.56e+04\n",
      "x103           0.0457    3.9e+04   1.17e-06      1.000   -7.64e+04    7.64e+04\n",
      "x104         238.9894   4.07e+05      0.001      1.000   -7.98e+05    7.98e+05\n",
      "x105          -0.0032    1.5e+04   -2.1e-07      1.000   -2.95e+04    2.95e+04\n",
      "x106           0.0157    8.2e+04   1.92e-07      1.000   -1.61e+05    1.61e+05\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(small.copy())\n",
    "y = small.distressed == 1\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results if this first model are bad. The low R2 and all P-values being 1 indicated that Logistic regerssion might not be a good fit for our data.\n",
    "\n",
    "There is a perfect spearation warning. This could be solved by using a different model (Firth logistic regression is usually recommended, but it's not part of the libraries I'm using). Another option would be to remove variables that are causing the bias. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrameMapper(drop_cols=[],\n",
      "                features=[(Index(['cashAndCashEquivalents', 'shortTermInvestments',\n",
      "       'cashAndShortTermInvestments', 'netReceivables',\n",
      "       'inventory_balance_sheet', 'otherCurrentAssets', 'totalCurrentAssets',\n",
      "       'propertyPlantEquipmentNet', 'goodwill', 'intangibleAssets',\n",
      "       ...\n",
      "       'incomeTaxExpense', 'netIncome_income_statement',\n",
      "       'netIncomeRatio_income_statement', 'eps', 'epsdiluted',\n",
      "       'weightedAverageShsOut', 'weightedAverageShsOutDil', 'distressed',\n",
      "       'year', 'marketcap'],\n",
      "      dtype='object', length=105),\n",
      "                           StandardScaler())])\n"
     ]
    }
   ],
   "source": [
    "print(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
