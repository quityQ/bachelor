{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, using scikit-learn and statsmodels (based on introduction to statistical learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import math\n",
    "import statsmodels.api as sm \n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor as VIF\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from ISLP.models import summarize\n",
    "from ISLP import confusion_table\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "large = pd.read_csv('../datasets/full_cleaned_dataset.csv')\n",
    "small = pd.read_csv('../datasets/1std_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping unnesacary columns (z_score column, and index columns, and year))\n",
    "small.drop(columns=['z_score', 'year'], inplace=True)\n",
    "large.drop(columns=['Unnamed: 0', 'year'], inplace=True)\n",
    "\n",
    "#drop any non-numeric colums from both datasets\n",
    "#getting lists of numeric columns\n",
    "numeric_columns = large.select_dtypes(include=np.number).columns\n",
    "\n",
    "#dropping non-numeric columns from large and small datasets\n",
    "large = large[numeric_columns]\n",
    "small = small[numeric_columns]\n",
    "\n",
    "#move the target (distressed) out of the dataset\n",
    "large_target = large.pop('distressed')\n",
    "small_target = small.pop('distressed')\n",
    "\n",
    "#turn dfs in numpy arrays\n",
    "nplarge = large.to_numpy()\n",
    "npsmall = small.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "scaler = StandardScaler()\n",
    "# standardize the features (mean 0, variance 1)\n",
    "X = scaler.fit_transform(npsmall.copy())\n",
    "y = small_target.copy()\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results if this first model are bad. The low R2 and all P-values being 1 indicated that Logistic regerssion might not be a good fit for our data.\n",
    "\n",
    "There is a perfect spearation warning. This could be solved by using a different model (Firth logistic regression is usually recommended, but it's not part of the libraries I'm using). Another option would be to remove variables that are causing the bias. It difficult to figure out which variables are causing the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There seem to be two issues with the data that must be solved:\n",
    "1. The data is incredibly inbalanced. There are only a few distressed observations (8 out of 667). Doing some resampling (oversampling the distressed observations) might help. This can be achieved with SMOTE\n",
    "2. The data seems to have multicollinearity. I should try to remove features that have high correlations, and build a new model based on the reduced dataset. This can be achieved by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#resampling using SMOTE\n",
    "small_res, y_res = SMOTE().fit_resample(X, y)\n",
    "\n",
    "print(f\"Original dataset shape {Counter(y)}\")\n",
    "print(f\"Resampled dataset shape {Counter(y_res)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a 50/50 split in the data regarding distressed observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glm = sm.GLM(y_res, small_res, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simply adding more distressed observations to the dataset didn't help with the model. The underlying issues with colinearity still should be present, even if there is no error regarding that anymore.\n",
    "\n",
    "To address that we can calculate the variance inflation factor (VIF) to find collinear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recreate a df with original features including the target variable\n",
    "vif_small = small.copy()\n",
    "vif_small['distressed'] = small_target.copy()\n",
    "\n",
    "# calculate VIF for each feature\n",
    "vals = [VIF(vif_small, i)\n",
    "        for i in range(1, vif_small.shape[1])]\n",
    "vif = pd.DataFrame({'vif': vals},\n",
    "                   index=vif_small.columns[1:])\n",
    "vif = vif.sort_values('vif', ascending=False)\n",
    "\n",
    "vif = vif['vif'].round(3)\n",
    "\n",
    "vif\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the VIF values, we can notice that some features end up with a VIF that converges into infinity. Those features can be removed from the dataset. (minorityInterest, totalLiabilitiesAndTotalEquity, totalEquity, totalLiabilitiesAndStockholdersEquity, totalStockholdersEquity, grossProfit, costOfRevenue, revenue)\n",
    "\n",
    "There are some features with very high VIFs, but for now we'll check those again, after the infinite VIFs have been removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the features with high VIF values\n",
    "toremove = ['minorityInterest', 'totalLiabilitiesAndTotalEquity', 'totalEquity',\n",
    "            'totalLiabilitiesAndStockholdersEquity', 'totalStockholdersEquity', 'grossProfit', 'costOfRevenue', 'revenue']\n",
    "\n",
    "for i in toremove:\n",
    "    if i in vif_small.columns:\n",
    "        vif_small.drop(columns=[i], inplace=True)\n",
    "    else:\n",
    "        print(f\"{i} not in columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calcualte VIFs again\n",
    "vals = [VIF(vif_small, i)\n",
    "        for i in range(1, vif_small.shape[1])]\n",
    "vif = pd.DataFrame({'vif': vals},\n",
    "                   index=vif_small.columns[1:])\n",
    "\n",
    "vif = vif.sort_values('vif', ascending=False)\n",
    "\n",
    "vif = vif['vif'].round(3)\n",
    "\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are still very high VIF values, but let's try to create a model with those features removed, to see if there are improvements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prepare dataset\n",
    "small2 = pd.read_csv('../datasets/1std_dataset.csv')\n",
    "small2.drop(columns=['z_score', 'year'], inplace=True)\n",
    "small2.drop(columns=toremove, inplace=True)\n",
    "\n",
    "#remove all non-numeric columns from the dataset\n",
    "numeric_columns = small2.select_dtypes(include=np.number).columns\n",
    "small2 = small2[numeric_columns]\n",
    "\n",
    "# create y, the target variable and X, the features\n",
    "X = small2.copy()\n",
    "X.drop(columns=['distressed'], inplace=True)\n",
    "y = small2.pop('distressed')\n",
    "\n",
    "# create dummy variables using smote\n",
    "X, y = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# run the logistic regression model\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is still very bad, let's try to remove all features with a VIF higher than 10. This will remove most of the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = [VIF(X, i)\n",
    "        for i in range(1, X.shape[1])]\n",
    "vif = pd.DataFrame({'vif': vals},\n",
    "                   index=X.columns[1:])\n",
    "\n",
    "vif.where(vif['vif'] > 10, inplace=True)\n",
    "vif.dropna(inplace=True)\n",
    "vif = vif.sort_values('vif', ascending=False)\n",
    "\n",
    "toremove2 = list(vif.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and prepare dataset\n",
    "small3 = pd.read_csv('../datasets/1std_dataset.csv')\n",
    "small3.drop(columns=['z_score', 'year'], inplace=True)\n",
    "small3.drop(columns=toremove, inplace=True)\n",
    "small3.drop(columns=toremove2, inplace=True)\n",
    "\n",
    "#remove all non-numeric columns from the dataset\n",
    "numeric_columns = small3.select_dtypes(include=np.number).columns\n",
    "small3 = small3[numeric_columns]\n",
    "\n",
    "# create y, the target variable and X, the features\n",
    "X = small3.copy()\n",
    "X.drop(columns=['distressed'], inplace=True)\n",
    "y = small3.pop('distressed')\n",
    "\n",
    "# create dummy variables using smote\n",
    "X, y = SMOTE().fit_resample(X, y)\n",
    "\n",
    "# run the logistic regression model\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We finally have a resemblence of a regular model.\n",
    "We can standardize the data, and try to run the model again, this should give a better readable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the features (mean 0, variance 1)\n",
    "X = scaler.fit_transform(X.copy())\n",
    "y = y.copy()\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "R2 is 0.4 which is not that great. Considering how many features we dropped. I would suggest that logistic regression is simply not a good fit for the data we have at hand. But let's do some checks with the model, to get more insights."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
