{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logistic regression, using scikit-learn and statsmodels (based on introduction to statistical learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import statsmodels.api as sm \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load datasets\n",
    "large = pd.read_csv('../datasets/full_cleaned_dataset.csv')\n",
    "small = pd.read_csv('../datasets/1std_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bicic\\AppData\\Local\\Temp\\ipykernel_11516\\317564394.py:16: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  small['distressed'] = target\n"
     ]
    }
   ],
   "source": [
    "#dropping the z_score column, so that small is a subset of large\n",
    "small.drop(columns=['z_score'], inplace=True) \n",
    "\n",
    "#drop any non-numeric colums from both datasets\n",
    "#getting lists of numeric columns\n",
    "numeric_columns = large.select_dtypes(include=np.number).columns\n",
    "\n",
    "#dropping non-numeric columns from large and small datasets\n",
    "large = large[numeric_columns]\n",
    "small = small[numeric_columns]\n",
    "\n",
    "#move the target (distressed) to the last column in both datasets\n",
    "target = large.pop('distressed')\n",
    "large['distressed'] = target\n",
    "target = small.pop('distressed')\n",
    "small['distressed'] = target\n",
    "\n",
    "#turn dfs in numpy arrays\n",
    "nplarge = large.to_numpy()\n",
    "npsmall = small.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(667, 106)\n"
     ]
    }
   ],
   "source": [
    "print(npsmall.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Generalized Linear Model Regression Results                  \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   No. Observations:                  667\n",
      "Model:                            GLM   Df Residuals:                      564\n",
      "Model Family:                Binomial   Df Model:                          102\n",
      "Link Function:                  Logit   Scale:                          1.0000\n",
      "Method:                          IRLS   Log-Likelihood:            -2.4329e-09\n",
      "Date:                Sat, 05 Apr 2025   Deviance:                   4.8659e-09\n",
      "Time:                        10:21:22   Pearson chi2:                 2.43e-09\n",
      "No. Iterations:                    27   Pseudo R-squ. (CS):             0.1219\n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1            -0.0010   1.56e+04  -6.63e-08      1.000   -3.06e+04    3.06e+04\n",
      "x2            -0.1470   6.38e+05   -2.3e-07      1.000   -1.25e+06    1.25e+06\n",
      "x3             0.0572   8.07e+04   7.09e-07      1.000   -1.58e+05    1.58e+05\n",
      "x4            -0.1767   6.37e+05  -2.77e-07      1.000   -1.25e+06    1.25e+06\n",
      "x5            -0.0173   1.01e+05  -1.72e-07      1.000   -1.98e+05    1.98e+05\n",
      "x6            -0.2658   6.26e+05  -4.24e-07      1.000   -1.23e+06    1.23e+06\n",
      "x7            -0.1535   2.78e+05  -5.52e-07      1.000   -5.44e+05    5.44e+05\n",
      "x8             0.2562    5.4e+05   4.75e-07      1.000   -1.06e+06    1.06e+06\n",
      "x9             0.0321   1.18e+05   2.73e-07      1.000   -2.31e+05    2.31e+05\n",
      "x10            0.0932   8.81e+04   1.06e-06      1.000   -1.73e+05    1.73e+05\n",
      "x11            0.0099    7.9e+04   1.26e-07      1.000   -1.55e+05    1.55e+05\n",
      "x12           -0.0485   1.33e+05  -3.64e-07      1.000   -2.61e+05    2.61e+05\n",
      "x13           -0.0429   1.08e+06  -3.96e-08      1.000   -2.12e+06    2.12e+06\n",
      "x14            0.0115   4.96e+05   2.32e-08      1.000   -9.72e+05    9.72e+05\n",
      "x15            0.1121   6.32e+05   1.77e-07      1.000   -1.24e+06    1.24e+06\n",
      "x16            0.1138      7e+05   1.62e-07      1.000   -1.37e+06    1.37e+06\n",
      "x17            0.0160   5.54e+05   2.89e-08      1.000   -1.09e+06    1.09e+06\n",
      "x18            0.2787   2.28e+06   1.22e-07      1.000   -4.46e+06    4.46e+06\n",
      "x19           -0.0068   1.03e+05  -6.65e-08      1.000   -2.01e+05    2.01e+05\n",
      "x20            0.0180   6.58e+04   2.73e-07      1.000   -1.29e+05    1.29e+05\n",
      "x21           -0.0026   4.72e+04  -5.53e-08      1.000   -9.25e+04    9.25e+04\n",
      "x22            0.0025   2.16e+04   1.17e-07      1.000   -4.24e+04    4.24e+04\n",
      "x23           -0.0066   7.38e+04     -9e-08      1.000   -1.45e+05    1.45e+05\n",
      "x24           -0.0440   9.56e+04   -4.6e-07      1.000   -1.87e+05    1.87e+05\n",
      "x25            0.0277   3.23e+05   8.59e-08      1.000   -6.32e+05    6.32e+05\n",
      "x26            0.0096   2.58e+04   3.71e-07      1.000   -5.05e+04    5.05e+04\n",
      "x27           -0.0151   3.78e+04  -3.99e-07      1.000    -7.4e+04     7.4e+04\n",
      "x28           -0.0534   1.61e+05  -3.31e-07      1.000   -3.16e+05    3.16e+05\n",
      "x29           -0.1485   4.64e+05   -3.2e-07      1.000    -9.1e+05     9.1e+05\n",
      "x30            0.0080   4.81e+05   1.67e-08      1.000   -9.43e+05    9.43e+05\n",
      "x31           -0.0020   3.07e+04   -6.4e-08      1.000   -6.02e+04    6.02e+04\n",
      "x32            0.0896   8.88e+05   1.01e-07      1.000   -1.74e+06    1.74e+06\n",
      "x33           -0.0198    3.9e+04  -5.09e-07      1.000   -7.63e+04    7.63e+04\n",
      "x34           -0.0142   3.73e+04  -3.79e-07      1.000   -7.31e+04    7.31e+04\n",
      "x35            0.0134   9.77e+04   1.37e-07      1.000   -1.91e+05    1.91e+05\n",
      "x36            0.0144   1.14e+05   1.27e-07      1.000   -2.23e+05    2.23e+05\n",
      "x37            0.0275   6.36e+04   4.32e-07      1.000   -1.25e+05    1.25e+05\n",
      "x38           -0.0226   8.49e+04  -2.66e-07      1.000   -1.66e+05    1.66e+05\n",
      "x39           -0.0224   8.41e+04  -2.67e-07      1.000   -1.65e+05    1.65e+05\n",
      "x40           -0.2037   9.56e+05  -2.13e-07      1.000   -1.87e+06    1.87e+06\n",
      "x41           -0.0006   3.97e+04  -1.53e-08      1.000   -7.78e+04    7.78e+04\n",
      "x42           -0.2037   9.56e+05  -2.13e-07      1.000   -1.87e+06    1.87e+06\n",
      "x43           -0.0035   1.14e+06  -3.07e-09      1.000   -2.22e+06    2.22e+06\n",
      "x44            0.2324   8.11e+05   2.87e-07      1.000   -1.59e+06    1.59e+06\n",
      "x45           -0.1961    5.8e+05  -3.38e-07      1.000   -1.14e+06    1.14e+06\n",
      "x46           -0.2151   9.69e+05  -2.22e-07      1.000    -1.9e+06     1.9e+06\n",
      "x47           -0.0602    2.5e+05  -2.41e-07      1.000    -4.9e+05     4.9e+05\n",
      "x48            0.0113   8.33e+04   1.36e-07      1.000   -1.63e+05    1.63e+05\n",
      "x49            0.0718   8.36e+05   8.59e-08      1.000   -1.64e+06    1.64e+06\n",
      "x50           -0.0756   2.76e+06  -2.74e-08      1.000   -5.42e+06    5.42e+06\n",
      "x51            0.0008    2.2e+05   3.65e-09      1.000   -4.31e+05    4.31e+05\n",
      "x52            0.0082   2.18e+05   3.77e-08      1.000   -4.28e+05    4.28e+05\n",
      "x53            0.0113   2.06e+05   5.46e-08      1.000   -4.04e+05    4.04e+05\n",
      "x54            0.0745   2.69e+06   2.77e-08      1.000   -5.27e+06    5.27e+06\n",
      "x55           -0.0597   8.34e+05  -7.15e-08      1.000   -1.63e+06    1.63e+06\n",
      "x56           -0.0047   2.26e+06  -2.08e-09      1.000   -4.44e+06    4.44e+06\n",
      "x57           -0.0027   2.76e+05   -9.6e-09      1.000   -5.42e+05    5.42e+05\n",
      "x58            0.0212   2.57e+05   8.25e-08      1.000   -5.04e+05    5.04e+05\n",
      "x59            0.1015   1.02e+06   9.95e-08      1.000      -2e+06       2e+06\n",
      "x60            0.0791   9.85e+05   8.03e-08      1.000   -1.93e+06    1.93e+06\n",
      "x61           -0.0204   1.08e+06   -1.9e-08      1.000   -2.11e+06    2.11e+06\n",
      "x62           -0.1312   1.15e+06  -1.15e-07      1.000   -2.24e+06    2.24e+06\n",
      "x63           -0.0224   1.07e+05   -2.1e-07      1.000   -2.09e+05    2.09e+05\n",
      "x64           -0.0208   1.41e+05  -1.48e-07      1.000   -2.76e+05    2.76e+05\n",
      "x65           -0.0106   7.88e+04  -1.34e-07      1.000   -1.54e+05    1.54e+05\n",
      "x66           -0.0139    1.3e+05  -1.07e-07      1.000   -2.55e+05    2.55e+05\n",
      "x67           -0.2063   2.44e+05  -8.46e-07      1.000   -4.78e+05    4.78e+05\n",
      "x68           -0.1080   4.16e+05   -2.6e-07      1.000   -8.15e+05    8.15e+05\n",
      "x69           -0.0043   3.57e+04  -1.21e-07      1.000      -7e+04       7e+04\n",
      "x70            0.2246   2.22e+07   1.01e-08      1.000   -4.35e+07    4.35e+07\n",
      "x71           -0.2229   6.08e+07  -3.67e-09      1.000   -1.19e+08    1.19e+08\n",
      "x72            0.2407   5.05e+07   4.77e-09      1.000    -9.9e+07     9.9e+07\n",
      "x73            0.6761   2.66e+07   2.54e-08      1.000   -5.21e+07    5.21e+07\n",
      "x74            0.1114   5.51e+06   2.02e-08      1.000   -1.08e+07    1.08e+07\n",
      "x75           -0.6158   2.49e+07  -2.47e-08      1.000   -4.88e+07    4.88e+07\n",
      "x76           -0.0434    8.6e+04  -5.05e-07      1.000   -1.69e+05    1.69e+05\n",
      "x77            0.0048   7.08e+04   6.76e-08      1.000   -1.39e+05    1.39e+05\n",
      "x78           -0.0631   1.32e+05   -4.8e-07      1.000   -2.58e+05    2.58e+05\n",
      "x79            0.0010   1.96e+04   5.17e-08      1.000   -3.83e+04    3.83e+04\n",
      "x80           -0.0225   8.27e+04  -2.72e-07      1.000   -1.62e+05    1.62e+05\n",
      "x81           -0.0012   4.73e+04  -2.45e-08      1.000   -9.27e+04    9.27e+04\n",
      "x82           -0.0084   3.32e+04  -2.53e-07      1.000   -6.51e+04    6.51e+04\n",
      "x83           -0.0638   1.46e+05  -4.36e-07      1.000   -2.87e+05    2.87e+05\n",
      "x84           -0.0512   1.54e+05  -3.33e-07      1.000   -3.02e+05    3.02e+05\n",
      "x85            0.0752   2.25e+05   3.34e-07      1.000   -4.41e+05    4.41e+05\n",
      "x86            0.0124   1.12e+05   1.11e-07      1.000   -2.19e+05    2.19e+05\n",
      "x87           -0.0827   3.48e+05  -2.38e-07      1.000   -6.82e+05    6.82e+05\n",
      "x88           -0.0348   3.43e+05  -1.01e-07      1.000   -6.72e+05    6.72e+05\n",
      "x89            0.0703   2.15e+05   3.27e-07      1.000   -4.21e+05    4.21e+05\n",
      "x90           -0.2827   6.25e+05  -4.52e-07      1.000   -1.23e+06    1.23e+06\n",
      "x91           -0.0420    6.6e+04  -6.36e-07      1.000   -1.29e+05    1.29e+05\n",
      "x92            0.0095   2.26e+05    4.2e-08      1.000   -4.44e+05    4.44e+05\n",
      "x93            0.0036   5.22e+04   6.98e-08      1.000   -1.02e+05    1.02e+05\n",
      "x94           -0.0061   1.13e+05  -5.36e-08      1.000   -2.22e+05    2.22e+05\n",
      "x95            0.2061   6.52e+05   3.16e-07      1.000   -1.28e+06    1.28e+06\n",
      "x96            0.0743   2.17e+05   3.42e-07      1.000   -4.26e+05    4.26e+05\n",
      "x97            0.0194   8.41e+04   2.31e-07      1.000   -1.65e+05    1.65e+05\n",
      "x98            0.1367   3.78e+05   3.62e-07      1.000    -7.4e+05     7.4e+05\n",
      "x99           -0.0344   2.34e+05  -1.47e-07      1.000   -4.59e+05    4.59e+05\n",
      "x100        -161.1562    2.5e+09  -6.46e-08      1.000   -4.89e+09    4.89e+09\n",
      "x101         161.3120    2.5e+09   6.46e-08      1.000   -4.89e+09    4.89e+09\n",
      "x102          -0.0227   2.25e+04  -1.01e-06      1.000   -4.42e+04    4.42e+04\n",
      "x103           0.0430   3.86e+04   1.11e-06      1.000   -7.57e+04    7.57e+04\n",
      "x104          -0.0051    1.5e+04  -3.41e-07      1.000   -2.95e+04    2.95e+04\n",
      "x105           0.0086    8.2e+04   1.05e-07      1.000   -1.61e+05    1.61e+05\n",
      "x106         238.9757   4.07e+05      0.001      1.000   -7.98e+05    7.98e+05\n",
      "==============================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n",
      "c:\\Users\\bicic\\Documents\\bachelor\\.venv\\Lib\\site-packages\\statsmodels\\genmod\\generalized_linear_model.py:1342: PerfectSeparationWarning: Perfect separation or prediction detected, parameter may not be identified\n",
      "  warnings.warn(msg, category=PerfectSeparationWarning)\n"
     ]
    }
   ],
   "source": [
    "# logistic regression\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(npsmall.copy())\n",
    "y = npsmall[:,-1]\n",
    "glm = sm.GLM(y, X, family=sm.families.Binomial())\n",
    "results = glm.fit()\n",
    "\n",
    "print(results.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results if this first model are bad. The low R2 and all P-values being 1 indicated that Logistic regerssion might not be a good fit for our data.\n",
    "\n",
    "There is a perfect spearation warning. This could be solved by using a different model (Firth logistic regression is usually recommended, but it's not part of the libraries I'm using). Another option would be to remove variables that are causing the bias. It difficult to figure out which variables are causing the bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
