{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yahoo finance python extension testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf \n",
    "dat = yf.Ticker(\"ADEN\")\n",
    "eu = yf.Market(\"EUROPE\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eu.summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scraping tests "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_income_statement(ticker): \n",
    "    url = f\"https://marketplace.financialmodelingprep.com/public/income-statement/{ticker}?period=annual&limit=4&apikey=\"\n",
    "        \n",
    "    response = requests.get(url).json()\n",
    "\n",
    "    with open(\"test.json\", \"w\") as f:\n",
    "        json.dump(response, f)\n",
    "\n",
    "\n",
    "get_income_statement(\"UHR.SW\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "companies = pd.read_csv(\"data\\equity_issuers.csv\", delimiter=\";\", encoding = \"ISO-8859-1\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, row in companies.iterrows():\n",
    "    name = row[\"Company\"]\n",
    "    ticker = row[\"Symbol\"] + \".SW\"\n",
    "    \n",
    "    \n",
    "    print(\"The ticker for \" + name + \" is \" + ticker)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = pd.read_json(r\"data\\balance_sheets\\Zug Estates Holding AG_balance_sheet_statement.json\")\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with all available balance sheets\n",
    "\n",
    "balance_sheets = pd.DataFrame()\n",
    "\n",
    "file_list = glob.glob(r\".\\data\\balance_sheets\\*.json\")\n",
    "\n",
    "dfs = [] # an empty list to store the data frames\n",
    "for file in file_list:\n",
    "    data = pd.read_json(file)\n",
    "    dfs.append(data)\n",
    "\n",
    "balance_sheets = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "balance_sheets.to_csv(\"datasets/balance_sheets.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with all available cash flow statements\n",
    "\n",
    "cash_flows = pd.DataFrame()\n",
    "\n",
    "file_list = glob.glob(rf\".\\data\\cash_flow_statements\\*.json\")\n",
    "\n",
    "dfs = [] # an empty list to store the data frames\n",
    "for file in file_list:\n",
    "    data = pd.read_json(file)\n",
    "    dfs.append(data)\n",
    "    \n",
    "cash_flows = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "cash_flows.to_csv(\"datasets/cash_flows.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating dataset with all available income statements\n",
    "\n",
    "income_statements = pd.DataFrame()\n",
    "\n",
    "file_list = glob.glob(rf\".\\data\\income_statements\\*.json\")\n",
    "\n",
    "dfs = []\n",
    "for file in file_list:\n",
    "    data = pd.read_json(file)\n",
    "    dfs.append(data)\n",
    "    \n",
    "income_statements = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "income_statements.to_csv(\"datasets/income_statements.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset playground"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "df = pd.read_csv(\"datasets/balance_sheets.csv\")\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO\n",
    "print(\"INFO\\n\")\n",
    "print(df.info())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "\n",
    "# STAT SUMMARY\n",
    "print(\"STAT SUMMARY\\n\")\n",
    "print(df.describe())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "# MISING VALUES\n",
    "print(\"MISSING VALUES\\n\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns='cik')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the altman z-score\n",
    "df[\"z_score\"] = 1.2*df[\"totalCurrentAssets\"]/df[\"totalAssets\"] + 1.4*df[\"retainedEarnings\"]/df[\"totalAssets\"] + 3.3*df[\"???ebit\"]/df[\"totalAssets\"] + 0.6*df[\"???marketCap\"]/df[\"totalLiabilities\"] + 0.999*df[\"???totalRevenue\"]/df[\"totalAssets\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read cash flows\n",
    "df_cf = pd.read_csv(\"datasets/cash_flow_statements.csv\")\n",
    "\n",
    "print(df_cf.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO\n",
    "print(\"INFO\\n\")\n",
    "print(df_cf.info())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "\n",
    "# STAT SUMMARY\n",
    "print(\"STAT SUMMARY\\n\")\n",
    "print(df_cf.describe())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "# MISING VALUES\n",
    "print(\"MISSING VALUES\\n\")\n",
    "print(df_cf.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read income statements\n",
    "df_is = pd.read_csv(\"datasets/income_statements.csv\")\n",
    "\n",
    "print(df_is.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO\n",
    "print(\"INFO\\n\")\n",
    "print(df_is.info())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "\n",
    "# STAT SUMMARY\n",
    "print(\"STAT SUMMARY\\n\")\n",
    "print(df_is.describe())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "# MISING VALUES\n",
    "print(\"MISSING VALUES\\n\")\n",
    "print(df_is.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = pd.read_csv(\"datasets/full_raw_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INFO\n",
    "print(\"INFO\\n\")\n",
    "print(df_full.info())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "\n",
    "# STAT SUMMARY\n",
    "print(\"STAT SUMMARY\\n\")\n",
    "print(df_full.describe())\n",
    "print(\"*\"*50+\"\\n\")\n",
    "\n",
    "# MISING VALUES\n",
    "print(\"MISSING VALUES\\n\")\n",
    "print(df_full.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_full.columns.tolist()\n",
    "for each in x:\n",
    "    print(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full_sample = df_full.head()\n",
    "df_full_sample = df_full_sample.transpose()\n",
    "\n",
    "df_full_sample.to_csv(\"raw_cleanup_guide.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleanup of full_raw_dataset\n",
    "refer to raw_cleanup_guide for details\n",
    "\n",
    "cleaning duplicates and unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"datasets/full_raw_dataset.csv\")\n",
    "\n",
    "# drop unnecessary columns \n",
    "df = df.drop(columns=[\"Unnamed: 0.1\", \"Unnamed: 0_x\", \"Unnamed: 0_y\", \"Unnamed: 0\", \"cik\", \"cik_y\", \"cik_x\", \"fillingDate\", \"fillingDate_x\", \"fillingDate_y\",\n",
    "             \"acceptedDate\", \"acceptedDate_x\", \"acceptedDate_y\", \"link\", \"link_x\", \"link_y\", \"finalLink\", \"finalLink_x\", \"finalLink_y\", \"distressed_y\", \"distressed_x\", \"period\", \"period_x\", \"period_y\"])\n",
    "\n",
    "# rename currency columns\n",
    "df.rename(columns={\"reportedCurrency_x\": \"currency_balance_sheet\", \"reportedCurrency_y\": \"currency_cash_flow_statement\", \"reportedCurrency\": \"currency_income_statement\"}, inplace=True)\n",
    "    \n",
    "# combine calendarYears to one column\n",
    "df['Year'] = df['calendarYear'].combine_first(df['calendarYear_x']).combine_first(df['calendarYear_y'])\n",
    "df = df.drop(columns=[\"calendarYear\", \"calendarYear_x\", \"calendarYear_y\"])\n",
    "\n",
    "# rename inventory columns\n",
    "df.rename(columns={\"inventory_x\": \"inventory_balance_sheet\", \"inventory_y\": \"inventory_cash_flow_statement\"}, inplace=True)\n",
    "\n",
    "# rename netIncome columns\n",
    "df.rename(columns={\"netIncome_x\": \"netIncome_cash_flow_statement\", \"netIncome_y\": \"netIncome_income_statement\", \"netIncomeRatio\": \"netIncomeRatio_income_statement\"}, inplace=True)\n",
    "\n",
    "# rename depreciationAndAmortization columns\n",
    "df.rename(columns={\"depreciationAndAmortization_x\": \"depreciationAndAmortization_cash_flow_statement\", \"depreciationAndAmortization_y\": \"depreciationAndAmortization_income_statement\"}, inplace=True)\n",
    "\n",
    "df.to_csv(\"datasets/full_cleaned_dataset.csv\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manuel duplicate checks\n",
    "\n",
    "#df['currencyComp_orig_x'] = np.where(df['reportedCurrency'] != df['reportedCurrency_x'], 1, 0)\n",
    "#df['currencyComp_orig_y'] = np.where(df['reportedCurrency'] != df['reportedCurrency_y'], 1, 0)\n",
    "df['dna_comp'] = np.where(df['depreciationAndAmortization_x'] != df['depreciationAndAmortization_y'], 1, 0)\n",
    "\n",
    "df.to_csv('temp.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create sample dataset to have an overview/guide for the analysis\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"datasets/full_cleaned_dataset.csv\")\n",
    "df_sample = df.sample(1)\n",
    "df_sample = df_sample.transpose()\n",
    "df_sample.to_csv(\"sample_dataset.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "webscraper tests for companiesmarketcap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)   \n",
    "\n",
    "if response.status_code != 200:\n",
    "    print(\"Failed to connect\")\n",
    " \n",
    "soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "table = soup.find('table', {'class': 'table'})\n",
    "\n",
    "headers = []   \n",
    "header_row = table.find('thead').find('tr') if table.find('thead') else table.find('tr')\n",
    "for th in header_row.find_all('th'):\n",
    "    headers.append(th.text.strip()) \n",
    "    \n",
    "    \n",
    "rows = []\n",
    "for row in table.find_all('tr'):\n",
    "    cells = row.find_all(['td', 'th'])\n",
    "    if len(cells) > 0:\n",
    "        rows.append([cell.text.strip() for cell in cells])\n",
    "        \n",
    "df = pd.DataFrame(rows)\n",
    "    \n",
    "df.to_csv(\"data/market_caps/abb_marketcap.csv\", header=None, index=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = pd.read_csv(\"data\\equity_issuers.csv\", delimiter=\";\", encoding = \"ISO-8859-1\") #encoding is necessary to match the encoding of the source file\n",
    "url = \"https://companiesmarketcap.com/search.do\"\n",
    "responses = []\n",
    "\n",
    "\n",
    "for i, row in companies.iterrows():\n",
    "    ticker = row[\"Symbol\"]\n",
    "    name = row[\"Company\"]\n",
    "    payload = {\n",
    "        'query': f\"{ticker}\"\n",
    "    }\n",
    "    \n",
    "    response = requests.post(url, data = payload)\n",
    "    responses.append(response)   \n",
    "    \n",
    "df = pd.DataFrame(responses)\n",
    "\n",
    "df.to_json(\"market_cap_tickersearch.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "filelist = glob.glob(\"data\\market_caps\\*.csv\")\n",
    "\n",
    "for i in filelist:\n",
    "    x = i.split(\"_\")[1].replace(\"caps\\\\\", \"\")\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge marketcap csv into full dataset\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"datasets/full_cleaned_dataset.csv\")\n",
    "df_mc = pd.read_csv(\"datasets/market_caps.csv\")\n",
    "df_mc = df_mc.drop(columns=[\"Change\"])\n",
    "\n",
    "df = pd.merge(df, df_mc, on=[\"symbol\", \"Year\"], how= \"left\") \n",
    "\n",
    "df.to_csv(\"temp.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qwe\n",
      "1    345\n",
      "0    322\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../datasets/1std_dataset.csv\")\n",
    "\n",
    "df['qwe'] = np.where(df['z_score'] >= 3.0, 1, 0)\n",
    "\n",
    "print(df['qwe'].value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
