{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dateset and display basic information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./datasets/full_cleaned_dataset.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcualte the Altman-Z score for each observation (where possible)\n",
    "And show the top and bottom 5 companies based on the Altman-Z score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"z_score\"] = 1.2*(df[\"totalCurrentAssets\"]/df[\"totalAssets\"]) + 1.4*df[\"retainedEarnings\"]/df[\"totalAssets\"] + 3.3 * (df[\"ebitda\"]-df[\"depreciationAndAmortization_income_statement\"])/df[\"totalAssets\"] + 0.6*df[\"marketcap\"] / df[\"totalLiabilities\"] + 0.999*df[\"revenue\"]/df[\"totalAssets\"]\n",
    "df[\"z_score\"] = df[\"z_score\"].round(2)\n",
    "df.dropna(subset=['z_score'], inplace=True)\n",
    "\n",
    "df_zscore_analysis = df[['symbol', 'year', 'z_score', 'distressed', 'totalCurrentAssets', 'totalAssets', 'retainedEarnings', 'ebitda', 'depreciationAndAmortization_income_statement', 'totalLiabilities', 'marketcap', 'revenue']]\n",
    "df_zscore_print = df[['symbol', 'year', 'z_score', 'distressed']]\n",
    "print(df_zscore_print.sort_values(by=['z_score'], ascending=False).head(10))\n",
    "print(df_zscore_print.sort_values(by=['z_score'], ascending=True).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Altman-Z score is usually interpreted as follows:\n",
    "- Z > 3.0: Safe zone\n",
    "- 1.8 < Z < 3.0: Grey zone\n",
    "- Z < 1.8: Distress zone\n",
    "\n",
    "Looking at the top and bottom 10, we get very extreme values (above 25 and below -6) which are likely due to the faulty or inaccurate data. These entries should be considered as outliers and removed for further analysis.\n",
    "Interestingly, in the bottom 10, we don't have companies that we considered to be distressed. In a next step let's take a look at the score of the distressed companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_zscore_print.where(df_zscore_print['distressed'] == 1).sort_values(by=['z_score'], ascending=False).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the the Altman-Z score does seem to be a decent indicator in the case of our distressed companies.\n",
    "CSGN (Credit Suisse) does have a very low score\n",
    "STLN (Swiss Steel Holding) had a low score in 2020, but it recovered in later years. Checking the news, it shows that the company went through rebranding, after some stressful years. In 2023 it dropped again, fitting with their wish to remove themselves from the stock market."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at the distribution just numerically and visually in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['z_score'].describe())  \n",
    "\n",
    "# Histogram\n",
    "bins =  int(math.sqrt(len(df.index))) # number of bins based on square root of number of data points\n",
    "plt.hist(df['z_score'], bins=bins, edgecolor='black')\n",
    "plt.title('Z-Score Distribution')\n",
    "plt.xlabel('Z-Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just another proof that there are extreme outliers in the data.\n",
    "Let's remove the outliers based the standard deviation (7.48) and see how the distribution looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "std = 7.48\n",
    "mean = 4.03\n",
    "\n",
    "df_1std = df.where((df['z_score'] >= mean - std) & (df['z_score'] <= mean + std)).dropna()\n",
    "df_2std = df.where((df['z_score'] >= mean - 2*std) & (df['z_score'] <= mean + 2*std)).dropna()\n",
    "\n",
    "print(df_1std['z_score'].describe())\n",
    "print(df_2std['z_score'].describe())\n",
    "\n",
    "bins1 = int(math.sqrt(len(df_1std.index)))\n",
    "bins2 = int(math.sqrt(len(df_2std.index)))\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharey=True, tight_layout=True)\n",
    "plt.ylabel('Frequency')\n",
    "axs[0].hist(df_1std['z_score'], bins=bins1, edgecolor='black')\n",
    "axs[0].set_title('1 Standard Deviation')\n",
    "axs[1].hist(df_2std['z_score'], bins=bins2, edgecolor='black')\n",
    "axs[1].set_title('2 Standard Deviations')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Staying within a single standard deviation the number of observations drop from 731 to 667. This is still a good number of observations to work with.\n",
    "The bad news is that the means is still very high at 3.49. The expected mean should be closer to 1.8, which is also the cutoff point for the grey zone according to Altman. However, considering that the data we're working with aren't just random companies, but sourced from a high perfoming index, this might be a reasonable result.\n",
    "\n",
    "Interesting are the companies with negative scores, let's take a look at them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_scores = df_1std.where(df_1std['z_score']<0).dropna()\n",
    "negative_z_score_analysis = negative_scores[['symbol', 'year', 'z_score', 'totalCurrentAssets', 'totalAssets', 'retainedEarnings', 'ebitda', 'depreciationAndAmortization_income_statement', 'totalLiabilities', 'marketcap', 'revenue']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_z_score_analysis['ebit'] = negative_z_score_analysis['ebitda'] - negative_z_score_analysis['depreciationAndAmortization_income_statement']\n",
    "negative_z_score_analysis = negative_z_score_analysis.drop(columns=['ebitda', 'depreciationAndAmortization_income_statement'])\n",
    "negative_z_score_analysis['A'] = negative_z_score_analysis['totalCurrentAssets'] / negative_z_score_analysis['totalAssets']\n",
    "negative_z_score_analysis['B'] = negative_z_score_analysis['retainedEarnings'] / negative_z_score_analysis['totalAssets']\n",
    "negative_z_score_analysis['C'] = negative_z_score_analysis['ebit'] / negative_z_score_analysis['totalAssets']\n",
    "negative_z_score_analysis['D'] = negative_z_score_analysis['marketcap'] / negative_z_score_analysis['totalLiabilities']\n",
    "negative_z_score_analysis['E'] = negative_z_score_analysis['revenue'] / negative_z_score_analysis['totalAssets']\n",
    "\n",
    "negative_z_score_analysis.to_csv('./datasamples/negative_z_score_analysis.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_data_raw = negative_z_score_analysis[['z_score', 'totalCurrentAssets', 'totalAssets', 'retainedEarnings', 'ebit', 'marketcap', 'totalLiabilities', 'revenue']]\n",
    "corr_data_z = negative_z_score_analysis[['z_score','A', 'B', 'C', 'D', 'E']]\n",
    "\n",
    "matrix1 = corr_data_raw.corr(numeric_only=True)\n",
    "matrix2 = corr_data_z.corr(numeric_only=True)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "sns.heatmap(matrix1, annot=True, ax=axs[0])\n",
    "sns.heatmap(matrix2, annot=True, ax=axs[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to matrices showing correlations between the Altman Z-Score and the datapoints of the companies with negative scores.\n",
    "The first matrix show the correlations of the datapoints as given in the dataset.\n",
    "The second matrix shows the correlations of the ratios used by the Altman Z-Score calculation.\n",
    "Here we see that 'B', the retained Earnings to total Assets ratio has the strongest correlation to the z-score.\n",
    "\n",
    "Looking at the data, we see that all retained earnings are negative in this subset.\n",
    "\n",
    "Let's create the second matrix using the entire dataset, and compare two matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_z_score_analysis = df_1std[['symbol', 'year', 'z_score', 'totalCurrentAssets', 'totalAssets', 'retainedEarnings', 'ebitda', 'depreciationAndAmortization_income_statement', 'totalLiabilities', 'marketcap', 'revenue']]\n",
    "complete_z_score_analysis['ebit'] = complete_z_score_analysis['ebitda'] - complete_z_score_analysis['depreciationAndAmortization_income_statement']\n",
    "complete_z_score_analysis = complete_z_score_analysis.drop(columns=['ebitda', 'depreciationAndAmortization_income_statement'])\n",
    "complete_z_score_analysis['A'] = complete_z_score_analysis['totalCurrentAssets'] / complete_z_score_analysis['totalAssets']\n",
    "complete_z_score_analysis['B'] = complete_z_score_analysis['retainedEarnings'] / complete_z_score_analysis['totalAssets']\n",
    "complete_z_score_analysis['C'] = complete_z_score_analysis['ebit'] / complete_z_score_analysis['totalAssets']\n",
    "complete_z_score_analysis['D'] = complete_z_score_analysis['marketcap'] / complete_z_score_analysis['totalLiabilities']\n",
    "complete_z_score_analysis['E'] = complete_z_score_analysis['revenue'] / complete_z_score_analysis['totalAssets']\n",
    "\n",
    "complete_corr_data = complete_z_score_analysis[['z_score','A', 'B', 'C', 'D', 'E']]\n",
    "\n",
    "complete_matrix = complete_corr_data.corr(numeric_only=True)\n",
    "negative_matrix = corr_data_z.corr(numeric_only=True)\n",
    "\n",
    "fig, axs = plt.subplots(ncols=2, figsize=(15, 5))\n",
    "sns.heatmap(complete_matrix, annot=True, ax=axs[0])\n",
    "sns.heatmap(negative_matrix, annot=True, ax=axs[1])\n",
    "axs[0].set_title('All Z-Score')\n",
    "axs[1].set_title('Negative Z-Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In comparison, the correlation of B is weaker in the full dataset, but it's still decently strong.\n",
    "The strongest correlation is now D, Market Cap to Total Liabilities ratio. Which I believe logically makes sense, as we're dealing with companies that have been sourced from a high performing index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# barchart of df_zscore_analysis\t\n",
    "df_zscore_analysis = df_zscore_analysis.sort_values(by=['z_score'], ascending=False)\n",
    "\n",
    "plt.bar(df_zscore_analysis['symbol'], df_zscore_analysis['z_score'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
